name: Crawl, Parse, and Deploy

on:
  schedule:
    - cron: '0 2 * * *' # 2am daily
  workflow_dispatch: # manual trigger

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  crawl:
    runs-on: ubuntu-latest
    continue-on-error: true # Allow one store to fail without stopping others?
    strategy:
      matrix:
        store: [albert, billa, globus, tesco, kupi]
        include:
          - store: albert
            crawl_args: --headless --workers 4 --limit 0
          - store: billa
            crawl_args: --headless --workers 4 --limit 0
          - store: globus
            crawl_args: --headless --workers 4 --limit 0
          - store: tesco
            crawl_args: --headless --workers 4
          - store: kupi
            crawl_args: --workers 4
            
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # Optional: Restore raw data cache if we want incremental crawls
      # But usually for a fresh daily crawl we might not want it?
      # The original script used cache for Tesco/Kupi.
      - name: Restore Raw Data Cache
        uses: actions/cache/restore@v4
        with:
          path: data/${{ matrix.store }}_raw
          key: ${{ runner.os }}-${{ matrix.store }}-raw-last
          restore-keys: |
            ${{ runner.os }}-${{ matrix.store }}-raw-

      - name: Run Crawler
        run: python sources/crawl_${{ matrix.store }}.py ${{ matrix.crawl_args }}

      - name: Kill lingering processes
        if: always()
        run: |
          pkill -f crawler.py || true
          pkill -f chrome || true

      - name: Save Raw Data Cache
        if: always()
        id: save_cache
        uses: actions/cache/save@v4
        with:
          path: data/${{ matrix.store }}_raw
          key: ${{ runner.os }}-${{ matrix.store }}-raw-${{ github.run_id }}

      - name: Cleanup Old Caches
        if: steps.save_cache.outcome == 'success'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PREFIX="${{ runner.os }}-${{ matrix.store }}-raw-"
          CURRENT_KEY="${{ runner.os }}-${{ matrix.store }}-raw-${{ github.run_id }}"
          
          echo "Fetching caches with prefix: $PREFIX"
          gh cache list --key "$PREFIX" --json id,key --limit 100 > caches.json
          
          jq -c '.[]' caches.json | while read -r cache; do
            ID=$(echo "$cache" | jq -r '.id')
            KEY=$(echo "$cache" | jq -r '.key')
            
            if [ "$KEY" != "$CURRENT_KEY" ]; then
              echo "Deleting old cache: $KEY (ID: $ID)"
              gh cache delete "$KEY" || true
            else
              echo "Keeping current cache: $KEY"
            fi
          done

      - name: Upload Raw Data
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: raw-${{ matrix.store }}
          path: data/${{ matrix.store }}_raw
          retention-days: 1

  parse:
    needs: crawl
    runs-on: ubuntu-latest
    strategy:
      matrix:
        store: [albert, billa, globus, tesco, kupi]
        include:
          # Use same matrix include logic to get parser args
          - store: albert
            parse_args: --workers 4
          - store: billa
            parse_args: --workers 4
          - store: globus
            parse_args: --workers 4
          - store: tesco
            parse_args:
          - store: kupi
            parse_args:
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Parser dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Download Raw Data
        uses: actions/download-artifact@v4
        with:
          name: raw-${{ matrix.store }}
          path: data/${{ matrix.store }}_raw

      - name: Run Parser
        run: python sources/parse_${{ matrix.store }}.py ${{ matrix.parse_args }}

      - name: Upload Result
        uses: actions/upload-artifact@v4
        with:
          name: result-${{ matrix.store }}
          path: data/${{ matrix.store }}.result.json

  collect:
    needs: parse
    runs-on: ubuntu-latest
    steps:
      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          pattern: result-*
          path: data
          merge-multiple: true

      - name: List Collected Files
        run: ls -R data/

      - name: Upload Final Artifact
        uses: actions/upload-artifact@v4
        with:
          name: all-results
          path: data/*.result.json

  build-deploy:
    needs: collect
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download Final Results
        uses: actions/download-artifact@v4
        with:
          name: all-results
          path: data
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'npm'
          cache-dependency-path: browser/package-lock.json

      - name: Install npm dependencies
        working-directory: browser
        run: npm ci

      - name: Build Browser App
        working-directory: browser
        run: npm run build

      - name: Upload Pages Artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: browser/dist

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
